{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To export to tf_lite we cant do it directly, so what we do is to export to ONNX and then to tf_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('./best.pt')\n",
    "model.export(format='tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.19  Python-3.10.4 torch-2.3.0+cu118 CPU (11th Gen Intel Core(TM) i7-11370H 3.30GHz)\n",
      "YOLOv9c summary (fused): 384 layers, 25326187 parameters, 0 gradients, 102.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'best.pt' with input shape (1, 3, 288, 288) BCHW and output shape(s) (1, 13, 1701) (49.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.1 opset 15...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.9s, saved as 'best.onnx' (96.8 MB)\n",
      "\n",
      "Export complete (5.6s)\n",
      "Results saved to \u001b[1mD:\\projects\\freelance\\saudi cairo uni student\\work part 2\\98 - Copy\u001b[0m\n",
      "Predict:         yolo predict task=detect model=best.onnx imgsz=288  \n",
      "Validate:        yolo val task=detect model=best.onnx imgsz=288 data=/kaggle/working/Tomata-Leaf-Disease-3/data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'best.onnx'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import onnx2tf\n",
    "\n",
    "model = YOLO('./best.pt')\n",
    "model.export(format='onnx',opset=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from ONNX to TF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"./best.onnx\")\n",
    "\n",
    "# Convert ONNX model to TensorFlow\n",
    "tf_rep = prepare(onnx_model)\n",
    "\n",
    "# Export the TensorFlow model to a temporary directory\n",
    "tf_rep.export_graph(\"./temp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# default settings without optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model has been successfully converted to TFLite and saved at ./tf_lite_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_model_path_directory = './temp/'\n",
    "tflite_model_path = './tf_lite_model.tflite'\n",
    "\n",
    "# Convert the TensorFlow model to TFLite with TF Select ops enabled\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path_directory)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # Enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(tflite_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"ONNX model has been successfully converted to TFLite and saved at {tflite_model_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized tf_lite version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model has been successfully converted to TFLite and saved at ./tf_lite_model_optimized.tflite\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_model_path = './temp/'\n",
    "tflite_model_path = './tf_lite_model_optimized.tflite'\n",
    "# Convert the TensorFlow model to TFLite with TF Select ops enabled\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # Enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "# Apply optimization for quantization\n",
    "\"there are different optimizations available, we're just using the default optimization\"\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(tflite_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"ONNX model has been successfully converted to TFLite and saved at {tflite_model_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_images:0', 'index': 0, 'shape': array([  1,   3, 288, 288]), 'shape_signature': array([  1,   3, 288, 288]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 1223, 'shape': array([   1,   13, 1701]), 'shape_signature': array([   1,   13, 1701]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tf_lite_model_optimized.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(input_details)\n",
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[     3.6731      18.563      22.119 ...       180.3      240.39      276.63]\n",
      "  [     7.5418      5.4746      7.5929 ...      263.46      260.07       261.6]\n",
      "  [     7.2051      33.413      38.729 ...      335.43      325.01      321.88]\n",
      "  ...\n",
      "  [ 6.7341e-06  1.6802e-06  3.8269e-07 ...  1.3927e-06    1.13e-06    1.01e-06]\n",
      "  [ 1.6404e-08  5.7363e-10   1.573e-10 ...  9.4366e-07  8.5718e-07  8.0111e-07]\n",
      "  [ 9.3082e-09  2.6777e-11  7.7509e-13 ...  1.0211e-06  9.2971e-07  8.7273e-07]]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare input data\n",
    "input_data = np.random.randn(1,3,288,288).astype(np.float32)\n",
    "\n",
    "# Set input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Process output data\n",
    "print(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 1701)\n"
     ]
    }
   ],
   "source": [
    "print(output_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
